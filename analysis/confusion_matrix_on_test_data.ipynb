{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodr0283\\.virtualenvs\\neuromatch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertForSequenceClassification\n",
    "\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b73d187f30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 31\n",
    "\n",
    "## Set the random seeds for Python and Torch\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id): #function to initalize the seeds for the workers of DataLoader\n",
    "    worker_seed = torch.initial_seed() %2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_test_split(df,features = 'input_ids',target = 'label',attention = 'attention_mask' ,test_size = 0.2,val_size = 0.2,gen_seed= g_seed):\n",
    "    \"\"\"Return two dataset objects of training and testing samples respectively\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing all the relevant columns \n",
    "        features (str, optional): DataFrame column correspondign to the feature components. Defaults to 'input_ids'.\n",
    "        target (str, optional): DataFrame column corresponding to the label/target . Defaults to 'label'.\n",
    "        attention (str, optional): DataFrame column corresponding to the attention tokens. Defaults to 'attention_mask'.\n",
    "        test_size (float, optional): Percent size assigned to testing. Defaults to 0.2.\n",
    "        val_size (float, optional): Percent size assigne to validation. Defaults to 0.2.\n",
    "        gen_seed(torch.Generator, optional): Generator for seeding the random split. Defaults to g_seed defined at the start.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Training,testing, and validation datasets objects respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Turn DataFrame into tensor objects and then into a dataset\n",
    "    X_label = torch.cat(df[features].to_list(),dim = 0) #tokenized data\n",
    "    X_attention = torch.cat(df[attention].to_list(),dim = 0) #whether its a word or padding\n",
    "    y = torch.tensor(df[target].to_list())\n",
    "    dataset = TensorDataset(X_label,X_attention,y)\n",
    "    \n",
    "    #Split into training and testing datasets\n",
    "    num_samps = df.shape[0]\n",
    "    num_test = int(num_samps*test_size)\n",
    "    num_val = int(num_samps*val_size)\n",
    "    num_train = num_samps - num_test - num_val\n",
    "    \n",
    "    train_data, test_data, val_data = random_split(dataset,[num_train,num_test,num_val],generator = gen_seed)\n",
    "    \n",
    "    return train_data,test_data,val_data\n",
    "\n",
    "def load_checkpoint_model(load_path = '10min_2023_07_checkpoint'):\n",
    "  \"\"\"Load model, optimizer, and system properties.\n",
    "\n",
    "  Args:\n",
    "      load_path (str) : path to load from.\n",
    "\n",
    "  Returns:\n",
    "      tuple : BERT model,torch.optim, curr_epoch, train_losses,val_losses, accuracies\n",
    "  \"\"\" \n",
    "  with open(f'data/{load_path}','rb') as fid:\n",
    "    checkpoint_dict = pickle.load(fid) \n",
    "      \n",
    "  empty_model = torch.load(f'data/{load_path}_model')\n",
    "  empty_optim = torch.load(f'data/{load_path}_optim')\n",
    "      \n",
    "  return empty_model,empty_optim\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: Running on cpu \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/modeling_data.zip'):\n",
    "    id = '1p3wPDlGq7fsIjiEV2hce90N-OI3YCOdw'\n",
    "    output = \"data/modeling_data.zip\"\n",
    "    gdown.download(id=id, output=output, quiet=False)\n",
    "    \n",
    "df = pd.read_pickle('data/modeling_data.zip')\n",
    "df.label = df.label.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path  = '58min_2023_07_info'\n",
    "data_path = '396390_2023_07_25_07_29min_summary_info'\n",
    "model = torch.load(f'data/{model_path}_model')\n",
    "with open(f'data/{data_path}','rb') as f:\n",
    "    info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, validation_dataset = custom_train_test_split(df,test_size = info['test_data_size'], val_size = info['val_data_size'])\n",
    "\n",
    "test_loader = DataLoader(test_dataset,batch_size = 2 , shuffle = False, num_workers = 0, worker_init_fn = seed_worker, generator = g_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running on cpu \n"
     ]
    }
   ],
   "source": [
    "device = set_device()\n",
    "model.eval()\n",
    "all_labels = np.array([0])\n",
    "all_preds = np.array([0])\n",
    "for test_iter,batch in enumerate(test_loader):\n",
    "    if test_iter > 3:\n",
    "        break\n",
    "    \n",
    "    feature = batch[0].to(device)\n",
    "    attention = batch[1].to(device)\n",
    "    labels = batch[2\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(feature,token_type_ids = None,attention_mask =  attention).detach().cpu().numpy()\n",
    "    y_preds = np.argmax(logits,axis = 1)\n",
    "    all_preds = np.append(all_preds,y_preds)\n",
    "    all_labels = np.append(all_labels,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9, 17]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m y_preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(np\u001b[39m.\u001b[39marray(all_logits))\n\u001b[0;32m      2\u001b[0m y_true \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(all_labels)\n\u001b[1;32m----> 4\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(y_true,y_preds)\n\u001b[0;32m      5\u001b[0m ConfusionMatrixDisplay(cm)\u001b[39m.\u001b[39mplot()\n",
      "File \u001b[1;32mc:\\Users\\rodr0283\\.virtualenvs\\neuromatch\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rodr0283\\.virtualenvs\\neuromatch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:326\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m    232\u001b[0m     {\n\u001b[0;32m    233\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    243\u001b[0m ):\n\u001b[0;32m    244\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    327\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    328\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[1;32mc:\\Users\\rodr0283\\.virtualenvs\\neuromatch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rodr0283\\.virtualenvs\\neuromatch\\lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9, 17]"
     ]
    }
   ],
   "source": [
    "y_preds = np.round(np.array(all_preds))\n",
    "y_true = np.array(all_labels)\n",
    "\n",
    "cm = confusion_matrix(y_true,y_preds)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,_ = roc_curve(y_true, np.array(all_logits))\n",
    "roc_display = RocCurveDisplay(fpr = fpr, tpr = tpr).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
