{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Collecting pytorch_pretrained_bert\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from gdown) (3.12.2)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pytorch_pretrained_bert) (1.13.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pytorch_pretrained_bert) (1.23.5)\n",
      "Collecting boto3 (from pytorch_pretrained_bert)\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/9a/ce/5cbaf62a9c496d6c54bdd7ec8cf8cf6b1c37f4743dc781f88ef9b46389a7/boto3-1.28.11-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.28.11-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting regex (from pytorch_pretrained_bert)\n",
      "  Obtaining dependency information for regex from https://files.pythonhosted.org/packages/a4/06/85618f80ae552ac309ead9702c6826edda27884e26e07fdc8fa93f283546/regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=0.4.1->pytorch_pretrained_bert) (68.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=0.4.1->pytorch_pretrained_bert) (0.40.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Collecting botocore<1.32.0,>=1.31.11 (from boto3->pytorch_pretrained_bert)\n",
      "  Obtaining dependency information for botocore<1.32.0,>=1.31.11 from https://files.pythonhosted.org/packages/b4/49/e36528ae5e73d472c516dba84059ebab1cda8bde507610b15282e091cae7/botocore-1.31.11-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.31.11-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->pytorch_pretrained_bert)\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.11->boto3->pytorch_pretrained_bert) (2.8.2)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.28.11-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.11-py3-none-any.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, safetensors, regex, jmespath, huggingface-hub, botocore, transformers, s3transfer, gdown, boto3, pytorch_pretrained_bert\n",
      "Successfully installed boto3-1.28.11 botocore-1.31.11 gdown-4.7.1 huggingface-hub-0.16.4 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 regex-2023.6.3 s3transfer-0.6.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gdown pytorch_pretrained_bert transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertForSequenceClassification\n",
    "\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting random seeds for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f46b65a15b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 31\n",
    "\n",
    "## Set the random seeds for Python and Torch\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id): #function to initalize the seeds for the workers of DataLoader\n",
    "    worker_seed = torch.initial_seed() %2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1690530bce2d4d8bb0595ec46b927bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a900323e7d4442f389f81528c4a7c9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d398a5dcd394888badba8add590454f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") #load pre-trained tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenize, pad, and tensorize the features\n",
    "#the apply returns a series of dictionaries, so we turn into a list -> DataFrame so that we can \n",
    "#store everything together efficiently\n",
    "def process_data_frame(input_df):\n",
    "    \"\"\"Process DataFrame to format required for Pytorch\n",
    "\n",
    "    Args:\n",
    "        input_df (pandas.DataFrame): DataFrame read from csv\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with tensor data\n",
    "    \"\"\"\n",
    "    tensor_df = pd.DataFrame(list(input_df['text'].apply(lambda x: my_tokenizer(x,truncation = True, \n",
    "                                                           max_length = 512, \n",
    "                                                           add_special_tokens= True,\n",
    "                                                           padding = 'max_length',\n",
    "                                                           return_tensors = 'pt',\n",
    "                                                           return_attention_mask = True))))\n",
    "    tensor_df['label'] = torch.tensor(input_df['label'].values) #tensorize labels\n",
    "    \n",
    "    return tensor_df\n",
    "\n",
    "#Turn the pandas dataframes into lists then tensors as shown in https://mccormickml.com/2019/07/22/BERT-fine-tuning/#31-bert-tokenizer\n",
    "def custom_train_test_split(df,features = 'input_ids',target = 'label',attention = 'attention_mask' ,test_size = 0.2,val_size = 0.2,gen_seed= g_seed):\n",
    "    \"\"\"Return two dataset objects of training and testing samples respectively\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing all the relevant columns \n",
    "        features (str, optional): DataFrame column correspondign to the feature components. Defaults to 'input_ids'.\n",
    "        target (str, optional): DataFrame column corresponding to the label/target . Defaults to 'label'.\n",
    "        attention (str, optional): DataFrame column corresponding to the attention tokens. Defaults to 'attention_mask'.\n",
    "        test_size (float, optional): Percent size assigned to testing. Defaults to 0.2.\n",
    "        val_size (float, optional): Percent size assigne to validation. Defaults to 0.2.\n",
    "        gen_seed(torch.Generator, optional): Generator for seeding the random split. Defaults to g_seed defined at the start.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Training,testing, and validation datasets objects respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Turn DataFrame into tensor objects and then into a dataset\n",
    "    X_label = torch.cat(df[features].to_list(),dim = 0) #tokenized data\n",
    "    X_attention = torch.cat(df[attention].to_list(),dim = 0) #whether its a word or padding\n",
    "    y = torch.tensor(df[target].to_list())\n",
    "    dataset = TensorDataset(X_label,X_attention,y)\n",
    "    \n",
    "    #Split into training and testing datasets\n",
    "    num_samps = df.shape[0]\n",
    "    num_test = int(num_samps*test_size)\n",
    "    num_val = int(num_samps*val_size)\n",
    "    num_train = num_samps - num_test - num_val\n",
    "    \n",
    "    train_data, test_data, val_data = random_split(dataset,[num_train,num_test,num_val],generator = gen_seed)\n",
    "    \n",
    "    return train_data,test_data,val_data\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: Running on cpu \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device\n",
    "\n",
    "def freeze_all_but_classifier(model, freeze_status = True):\n",
    "  \"\"\"Freeze all the layers but the classification layer\n",
    "\n",
    "  Args:\n",
    "      model (BERT model): model to freeze layers on \n",
    "      freeze_status (bool, optional): Sets the requires_grad of all the previous layers to this value. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "      BERT model: the model that we are using, with layers frozen or not depending on do.\n",
    "  \"\"\"\n",
    "  for name,tensors in model.named_parameters():\n",
    "   if 'classifier' not in name:\n",
    "      tensors.requires_grad = freeze_status\n",
    "    \n",
    "  return model\n",
    "\n",
    "def save_checkpoint_model(empty_model,empty_optim,curr_epoch=None,train_losses=None,val_losses=None,accuracies=None,save_path = '10min_2023_07_checkpoint'):\n",
    "  \"\"\"Save the model, optimizer, and current properties of the system.\n",
    "\n",
    "  Args:\n",
    "      model (BERT model) : model to save into.\n",
    "      optimizer (torch.optim) : optimizer to save into.\n",
    "      curr_epoch (int) : epoch to save on.\n",
    "      train_losses (list) : list of training losses so far calculated.\n",
    "      val_losses (list) : list of validation losses so far calculated.\n",
    "      accuracies (list) : list of accuracies so far calculated.\n",
    "      save_path (str) : path to save to.\n",
    "\n",
    "  Returns:\n",
    "      tuple : BERT model,torch.optim, curr_epoch, train_losses,val_losses, accuracies\n",
    "  \"\"\"  \n",
    "  checkpoint_dict = {'curr_epoch':curr_epoch,'train_losses':train_losses,\n",
    "                       'val_losses':val_losses,'accuracies':accuracies}\n",
    "    \n",
    "  with open(f'data/{save_path}','wb') as fid:\n",
    "    pickle.dump(checkpoint_dict,fid,protocol = pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "  torch.save(empty_model,f'data/{save_path}_model') #save model \n",
    "  torch.save(empty_optim,f'data/{save_path}_optim') #save optimization method\n",
    "  return\n",
    "  \n",
    "def load_checkpoint_model(load_path = '10min_2023_07_checkpoint'):\n",
    "  \"\"\"Load model, optimizer, and system properties.\n",
    "\n",
    "  Args:\n",
    "      load_path (str) : path to load from.\n",
    "\n",
    "  Returns:\n",
    "      tuple : BERT model,torch.optim, curr_epoch, train_losses,val_losses, accuracies\n",
    "  \"\"\" \n",
    "  with open(f'data/{load_path}','rb') as fid:\n",
    "    checkpoint_dict = pickle.load(fid) \n",
    "      \n",
    "  empty_model = torch.load(f'data/{load_path}_model')\n",
    "  empty_optim = torch.load(f'data/{load_path}_optim')\n",
    "      \n",
    "  return empty_model,empty_optim,checkpoint_dict['curr_epoch'],checkpoint_dict['train_losses'],checkpoint_dict['val_losses'],checkpoint_dict['accuracies']\n",
    "  \n",
    "\n",
    "def generate_random_id():\n",
    "  \"\"\"Generates a random ID of 6 digits.\"\"\"\n",
    "  # id_digits = []\n",
    "  # for _ in range(6):\n",
    "  #   id_digits.append(str(random.randint(0, 9)))\n",
    "  # return ''.join(id_digits)\n",
    "  return str(datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1p3wPDlGq7fsIjiEV2hce90N-OI3YCOdw\n",
      "From (redirected): https://drive.google.com/uc?id=1p3wPDlGq7fsIjiEV2hce90N-OI3YCOdw&confirm=t&uuid=2f840c05-6d2f-4ee3-81c6-43c8900d9894\n",
      "To: /home/jupyter/neuromatch_mental_health/data/modeling_data.zip\n",
      "100%|██████████| 94.2M/94.2M [00:06<00:00, 15.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('data/modeling_data.zip'):\n",
    "    id = '1p3wPDlGq7fsIjiEV2hce90N-OI3YCOdw'\n",
    "    output = \"data/modeling_data.zip\"\n",
    "    gdown.download(id=id, output=output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/modeling_data.zip'): #if the data has not been processed into tensors, read from csv and process\n",
    "    df = pd.read_csv('data/cleandata.zip') #load dataset\n",
    "    df.dropna(inplace = True) #drop nans (4 samples)\n",
    "    \n",
    "    df = process_data_frame(df)\n",
    "    df.to_pickle('data/modeling_data.zip')\n",
    "else: #read from pickle object if it has already been processed\n",
    "    df = pd.read_pickle('data/modeling_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = df.label.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_batches = 300 #If we just want to do a subset of batches. Equal to np.nan to ignore\n",
    "batch_size = 260\n",
    "lr = 0.001 # increasing to make use of the scheduler\n",
    "eps = 1e-8\n",
    "epochs = 6 #The BERT authors recommend between 2 and 4. Adjust to 2 if overfitting.\n",
    "freeze_layers = False #Set to True if we want to freeze all the layers but the classification layer\n",
    "batch_n_report = 6 #Report every this amount of batches\n",
    "test_data_size = 0.15\n",
    "val_data_size = 0.15\n",
    "\n",
    "info = {'max_batches':max_batches,'batch_size':batch_size,'lr':lr,'eps':eps,\n",
    "        'epochs':epochs,'freeze_layers':freeze_layers,'seed':seed,\n",
    "        'val_data_size':val_data_size,'test_data_size':test_data_size}\n",
    "\n",
    "# generates a simple random id per execution\n",
    "execution_id = generate_random_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, validation_dataset = custom_train_test_split(df,test_size = test_data_size, val_size = val_data_size)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,batch_size = batch_size , shuffle = False, num_workers = 0, worker_init_fn = seed_worker, generator = g_seed)\n",
    "train_loader = DataLoader(train_dataset,batch_size = batch_size , drop_last = True, shuffle = True, worker_init_fn = seed_worker, generator = g_seed)\n",
    "validation_loader = DataLoader(validation_dataset,batch_size = batch_size , shuffle = False, num_workers = 0, worker_init_fn = seed_worker, generator = g_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [00:18<00:00, 22545517.55B/s]\n"
     ]
    }
   ],
   "source": [
    "this_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
    "this_model = freeze_all_but_classifier(this_model,freeze_status = freeze_layers)\n",
    "this_optim = my_optim = AdamW(this_model.parameters(),\n",
    "                              lr=lr,\n",
    "                              eps = eps)\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. NOT the total number of samples\n",
    "if max_batches == np.nan:\n",
    "    scheduler_total_steps = len(train_loader) * epochs \n",
    "else:\n",
    "    scheduler_total_steps = max_batches * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(my_optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = scheduler_total_steps)\n",
    "\n",
    "info['scheduler_total_steps'] = scheduler_total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(model,\n",
    "               optimizer,\n",
    "               train_dataset,\n",
    "               test_dataset,\n",
    "               val_dataset = [],\n",
    "               epochs = 4,\n",
    "               device = 'cpu',\n",
    "               report_every_n_batches = 10,\n",
    "               max_batch = np.nan,\n",
    "               load = ''): \n",
    "    \"\"\"Training loop for the model that outputs a loss\n",
    "\n",
    "    Args:\n",
    "        model (torch.model): Output of model has to be a loss, not labels\n",
    "        optimizer (torch.optimizer): Optimizer with the model parameters already fed in\n",
    "        train_dataset (torch.DataLoader) : Training dataset that has 3 outputs (features, attention, labels)\n",
    "        test_dataset (torch.DataLoader) : Test dataset that has 3 outputs (features, attention, labels)\n",
    "        val_dataset (torch.DataLoader,optional): Validation dataset that has 3 outputs (features, attention, labels). Defaults to [].\n",
    "        epochs (int, optional): Number of epochs to reiterate through batches. Defaults to 4.\n",
    "        device (str, optional): Device to load the data unto. Defaults to 'cpu'.\n",
    "        report_every_n_batches (int, optional): Number of epochs to update the loss everytime. Defaults to 10.\n",
    "        max_batch(int, optional): Max number of batches to asses. Defaults to np.nan.\n",
    "        load(str,optional): Path to load a checkpoint model. Defaults to ''.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Training loss, Validation Loss, Accuracies, total time\n",
    "    \"\"\"\n",
    "  \n",
    "    ts = time.time()\n",
    "    append = ''\n",
    "    if max_batch is not np.nan:\n",
    "        append  = f'.\\nOnly doing {max_batch} batches.'\n",
    "        \n",
    "    print(f'Begin model Training. \\nTotal epochs: {epochs}.\\nTotal training batches: {len(train_dataset)}.\\\n",
    "          \\nTotal validation batches: {len(val_dataset)}.\\nTotal testing batches: {len(test_dataset)}' + append + '\\n')\n",
    "    \n",
    "    all_train_loss, all_eval_loss, all_accuracy = [],[],[]\n",
    "    \n",
    "    if load != '': #If we did early stopping, we can resume \n",
    "        model,optimizer,start_epoch,all_train_loss,all_eval_loss,all_accuracy = load_checkpoint_model(load_path = load)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for ep in range(start_epoch,epochs): #iterate the desired epochs\n",
    "        tb = time.time()\n",
    "        \n",
    "        print(f'------------------- Epoch {ep} ------------------- ')\n",
    "        train_loss,eval_loss,accuracy = 0 ,0 ,0\n",
    "        \n",
    "        #Perform training of the model\n",
    "        model.train()\n",
    "        for train_iter,batch in enumerate(train_dataset): #iterate through all batches\n",
    "            t0 = time.time()    \n",
    "            if max_batch is not np.nan and train_iter > max_batch: #in case we want to do a batch_subset\n",
    "                break\n",
    "            \n",
    "            feature = batch[0].to(device) #extract relevant values and move to device\n",
    "            attention = batch[1].to(device)\n",
    "            label = batch[2].to(device)\n",
    "            \n",
    "            optimizer.zero_grad() #zero the gradients\n",
    "            loss = model(feature,token_type_ids = None, attention_mask = attention, labels = label)\n",
    "            \n",
    "            train_loss += loss.detach().cpu().numpy() #calculate epoch loss and detach\n",
    "            \n",
    "            loss.backward() #estimate the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #clip gradients to avoid \"exploding gradients\"\n",
    "            optimizer.step() #adjust parameters\n",
    "            scheduler.step() #adjust the scheduler\n",
    "            \n",
    "            if train_iter % report_every_n_batches == 0 and not train_iter ==0: #update output every 10 batches\n",
    "                elapsed_t = round(time.time() - t0,2)\n",
    "                print(f'Batch {train_iter} done in {elapsed_t} seconds. Total training loss: {np.round(float(train_loss),4)}')\n",
    "        \n",
    "        #Perform validation of the model\n",
    "        model.eval()\n",
    "        for eval_iter,batch in enumerate(val_dataset):\n",
    "            if max_batch is not np.nan and eval_iter > max_batch: #in case we want to do a batch_subset\n",
    "                break\n",
    "            \n",
    "            feature = batch[0].to(device) #extract relevant values and move to device\n",
    "            attention = batch[1].to(device)\n",
    "            label = batch[2].to(device)\n",
    "            \n",
    "            with torch.no_grad(): #we don't want to estimate gradients when validating\n",
    "                loss = model(feature,token_type_ids = None, attention_mask = attention, labels = label)\n",
    "            \n",
    "            eval_loss += loss.detach().cpu().numpy() #detach from tensors and turn into numpy arrays to plot and calculate\n",
    "        \n",
    "        number_test_batches = 0\n",
    "        \n",
    "        for test_iter,batch in enumerate(test_dataset):\n",
    "            if max_batch is not np.nan and test_iter > max_batch: #in case we want to do a batch subset\n",
    "                break\n",
    "                \n",
    "            feature = batch[0].to(device)\n",
    "            attention = batch[1].to(device)\n",
    "            labels = batch[2]\n",
    "            \n",
    "            with torch.no_grad(): #we don't want to estimate gradients when validating\n",
    "                logits = model(feature,token_type_ids = None, attention_mask = attention)\n",
    "                \n",
    "            # logits.detach().to('cpu').numpy()\n",
    "            preds = np.argmax(logits.detach().cpu().numpy(),axis = 1) #find the maximum probits\n",
    "            \n",
    "            accuracy  += np.sum(preds == labels.detach().cpu().numpy())/ len(labels.detach().cpu().numpy()) #find percent of accurate predictions\n",
    "            number_test_batches += 1\n",
    "        \n",
    "        accuracy = accuracy / number_test_batches * 100\n",
    "                             \n",
    "        all_train_loss.append(train_loss/train_iter)\n",
    "        all_eval_loss.append(eval_loss/eval_iter) #append losses to visualize across epochs\n",
    "        all_accuracy.append(accuracy)\n",
    "        \n",
    "        save_checkpoint_model(model,\n",
    "                         optimizer,\n",
    "                         curr_epoch=ep,\n",
    "                         train_losses=all_train_loss,\n",
    "                         val_losses=all_eval_loss,\n",
    "                         accuracies=all_accuracy,\n",
    "                         save_path = f'{datetime.now().strftime(\"%Mmin_%Y_%m_info\")}')\n",
    "        \n",
    "        \n",
    "        t_ep = round(time.time() - tb,2)\n",
    "        print(f'\\nEpoch {ep} done in {t_ep} seconds.\\nTotal training loss: {np.round(float(train_loss),4)}, Total validation loss: {np.round(float(eval_loss),4)}')\n",
    "        print(f'Averaged across batches: Training loss: {np.round(train_loss / train_iter,4)}, Validation loss: {np.round(eval_loss / eval_iter,4)}')\n",
    "        print(f'Accuracy on Testing Dataset for this epoch: {np.round(accuracy,2)}%\\n')\n",
    "      \n",
    "    total_time = time.time() - ts\n",
    "    print(f'-------------------  DONE TRAINING -------------------------------------- \\n total time: {round(total_time,2)} seconds')\n",
    "    return all_train_loss,all_eval_loss,all_accuracy,total_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled in this notebook.\n",
      "Begin model Training. \n",
      "Total epochs: 6.\n",
      "Total training batches: 800.          \n",
      "Total validation batches: 172.\n",
      "Total testing batches: 172.\n",
      "Only doing 300 batches.\n",
      "\n",
      "------------------- Epoch 0 ------------------- \n",
      "Batch 6 done in 10.39 seconds. Total training loss: 4.8397\n",
      "Batch 12 done in 10.35 seconds. Total training loss: 8.7055\n",
      "Batch 18 done in 10.49 seconds. Total training loss: 12.3906\n",
      "Batch 24 done in 10.46 seconds. Total training loss: 16.0359\n",
      "Batch 30 done in 10.44 seconds. Total training loss: 19.6512\n",
      "Batch 36 done in 10.42 seconds. Total training loss: 23.1472\n",
      "Batch 42 done in 10.4 seconds. Total training loss: 26.5797\n",
      "Batch 48 done in 10.46 seconds. Total training loss: 30.213\n",
      "Batch 54 done in 10.42 seconds. Total training loss: 33.8081\n",
      "Batch 60 done in 10.44 seconds. Total training loss: 37.3333\n",
      "Batch 66 done in 10.45 seconds. Total training loss: 40.8351\n",
      "Batch 72 done in 10.46 seconds. Total training loss: 44.3189\n",
      "Batch 78 done in 10.42 seconds. Total training loss: 47.7927\n",
      "Batch 84 done in 10.47 seconds. Total training loss: 51.2477\n",
      "Batch 90 done in 10.4 seconds. Total training loss: 54.5381\n",
      "Batch 96 done in 10.39 seconds. Total training loss: 58.0267\n",
      "Batch 102 done in 10.37 seconds. Total training loss: 61.5359\n",
      "Batch 108 done in 10.44 seconds. Total training loss: 64.9775\n",
      "Batch 114 done in 10.4 seconds. Total training loss: 68.4229\n",
      "Batch 120 done in 10.42 seconds. Total training loss: 71.8263\n",
      "Batch 126 done in 10.44 seconds. Total training loss: 75.2631\n",
      "Batch 132 done in 10.45 seconds. Total training loss: 78.6422\n",
      "Batch 138 done in 10.48 seconds. Total training loss: 82.0501\n",
      "Batch 144 done in 10.4 seconds. Total training loss: 85.3638\n",
      "Batch 150 done in 10.39 seconds. Total training loss: 88.8094\n",
      "Batch 156 done in 10.44 seconds. Total training loss: 92.1365\n",
      "Batch 162 done in 10.47 seconds. Total training loss: 95.461\n",
      "Batch 168 done in 10.41 seconds. Total training loss: 98.9369\n",
      "Batch 174 done in 10.42 seconds. Total training loss: 102.3775\n",
      "Batch 180 done in 10.41 seconds. Total training loss: 105.7083\n",
      "Batch 186 done in 10.45 seconds. Total training loss: 108.9657\n",
      "Batch 192 done in 10.36 seconds. Total training loss: 112.3027\n",
      "Batch 198 done in 10.52 seconds. Total training loss: 115.6774\n",
      "Batch 204 done in 10.42 seconds. Total training loss: 118.9197\n",
      "Batch 210 done in 10.4 seconds. Total training loss: 122.2945\n",
      "Batch 216 done in 10.42 seconds. Total training loss: 125.5888\n",
      "Batch 222 done in 10.39 seconds. Total training loss: 128.8423\n",
      "Batch 228 done in 10.41 seconds. Total training loss: 132.1404\n",
      "Batch 234 done in 10.41 seconds. Total training loss: 135.3815\n",
      "Batch 240 done in 10.45 seconds. Total training loss: 138.7854\n",
      "Batch 246 done in 10.4 seconds. Total training loss: 142.0602\n",
      "Batch 252 done in 10.41 seconds. Total training loss: 145.2409\n",
      "Batch 258 done in 10.42 seconds. Total training loss: 148.55\n",
      "Batch 264 done in 10.45 seconds. Total training loss: 151.8702\n",
      "Batch 270 done in 10.42 seconds. Total training loss: 155.1407\n",
      "Batch 276 done in 10.47 seconds. Total training loss: 158.4128\n",
      "Batch 282 done in 10.45 seconds. Total training loss: 161.5456\n",
      "Batch 288 done in 10.43 seconds. Total training loss: 164.9369\n",
      "Batch 294 done in 10.41 seconds. Total training loss: 168.2414\n",
      "Batch 300 done in 10.46 seconds. Total training loss: 171.6079\n",
      "\n",
      "Epoch 0 done in 6583.28 seconds.\n",
      "Total training loss: 171.6079, Total validation loss: 92.2667\n",
      "Averaged across batches: Training loss: 0.5701, Validation loss: 0.5396\n",
      "Accuracy on Testing Dataset for this epoch: 74.71%\n",
      "\n",
      "------------------- Epoch 1 ------------------- \n",
      "Batch 6 done in 10.44 seconds. Total training loss: 3.9108\n",
      "Batch 12 done in 10.38 seconds. Total training loss: 7.0866\n",
      "Batch 18 done in 10.48 seconds. Total training loss: 10.3172\n",
      "Batch 24 done in 10.43 seconds. Total training loss: 13.5908\n",
      "Batch 30 done in 10.44 seconds. Total training loss: 16.8664\n",
      "Batch 36 done in 10.43 seconds. Total training loss: 20.1221\n",
      "Batch 42 done in 10.42 seconds. Total training loss: 23.4383\n",
      "Batch 48 done in 10.43 seconds. Total training loss: 26.812\n",
      "Batch 54 done in 10.44 seconds. Total training loss: 30.0943\n",
      "Batch 60 done in 10.43 seconds. Total training loss: 33.4132\n",
      "Batch 66 done in 10.45 seconds. Total training loss: 36.7019\n",
      "Batch 72 done in 10.41 seconds. Total training loss: 39.9487\n",
      "Batch 78 done in 10.44 seconds. Total training loss: 43.1048\n",
      "Batch 84 done in 10.45 seconds. Total training loss: 46.3194\n",
      "Batch 90 done in 10.38 seconds. Total training loss: 49.565\n",
      "Batch 96 done in 10.46 seconds. Total training loss: 52.8616\n",
      "Batch 102 done in 10.4 seconds. Total training loss: 56.1361\n",
      "Batch 108 done in 10.47 seconds. Total training loss: 59.513\n",
      "Batch 114 done in 10.44 seconds. Total training loss: 62.7692\n",
      "Batch 120 done in 10.4 seconds. Total training loss: 66.0367\n",
      "Batch 126 done in 10.4 seconds. Total training loss: 69.2842\n",
      "Batch 132 done in 10.4 seconds. Total training loss: 72.5474\n",
      "Batch 144 done in 10.41 seconds. Total training loss: 78.8907\n"
     ]
    }
   ],
   "source": [
    "DEVICE = set_device()\n",
    "train_loss,eval_loss,accuracies,total_time = train_loop(this_model,\n",
    "                                             this_optim,\n",
    "                                             train_loader,\n",
    "                                             test_loader,\n",
    "                                             val_dataset = validation_loader,\n",
    "                                             epochs = epochs,\n",
    "                                             report_every_n_batches= batch_n_report,\n",
    "                                             device = DEVICE,\n",
    "                                             max_batch = max_batches)\n",
    "\n",
    "info.update({'avg_train_loss':train_loss,'avg_eval_loss':eval_loss,'accuracies':accuracies,'total_time':total_time})\n",
    "f_name_to_save = f'executions/{execution_id}_{datetime.now().strftime(\"%Y_%m_%d_%H_%Mmin_summary_info\")}'\n",
    "with open(f_name_to_save,'wb') as fid: #save information from training and validation as well as parameters\n",
    "    pickle.dump(info,fid,protocol = pickle.HIGHEST_PROTOCOL)\n",
    "torch.save(this_model,f'{f_name_to_save}_model')\n",
    "torch.save(this_optim,f'{f_name_to_save}_optim')\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 1, sharex = True, figsize = (8,8))\n",
    "\n",
    "ax[0].plot(range(epochs),train_loss,color = 'black',linestyle = 'solid',marker = 'o',label = 'Training Loss')\n",
    "ax[0].plot(range(epochs),eval_loss,color = 'black',linestyle = 'dashed',marker = 'o',fillstyle = 'none',label = 'Validation Loss')\n",
    "ax[0].set_ylabel('Average Loss Across Batches')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(range(epochs),accuracies,color = 'black',linestyle = 'solid',marker = 'o')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy on Testing Dataset');\n",
    "\n",
    "fig.savefig('executions/{execution_id}_curves.png')   # save the figure to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To read file:\n",
    "# with open('executions/22min_2023_07_info_summary','rb') as rfid:\n",
    "#    info = pickle.load(rfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m109"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
